# Softmax回归  
逻辑回归只能进行二分类，输出只能为0或1, 而Softmax回归能够解决多分类问题，可以看做是逻辑回归的扩展。  
大写的C来表示类别总个数，用0, 1, 2, ..., C-1指示不同类别。对应的，神经网络有Ｃ个输出，每个输出值都是一种概率，代表它属于某一类的可能性，各个输出值的概率之和为１。  

在神经网络的最后一层，像往常一样先计算出线性部分$$z$$，然后应用Softmax激活函数对各元素求幂再归一化，具体是:计算临时变量$$t=e^z$$ ,　跟$$z$$一样是$$C \times 1$$维 那么每个输出值为$$a_i = \frac{t_i}{\sum_{i=1}^{C} t_i} = \frac{e^{z_i}}{\sum_{i=1}^{C} e^{z_i}}$$  
例如，输出层计算出的$$z^{[l]}$$为$$\begin{bmatrix} 5 \\ 2 \\ -1 \\ 3 \end{bmatrix}$$，表明我们有四个分类$$C=4$$，$$z^{[l]}$$是4×1维向量，我们计算了临时变量$$t = \begin{bmatrix} e^{5} \\ e^{2} \\ e^{-1} \\ e^{3} \end{bmatrix}$$，对元素进行幂运算，输出层的激活函数$$g^{[L]}$$使用Softmax激活函数，那么输出就会是这样的:  
$$  a^{[L]} =  g^{[L]}(Z^{[L]}) = \begin{bmatrix} e^5/(e^5+e^2+e^{-1}+e^3)  \\ e^2/(e^5+e^2+e^{-1}+e^3) \\ e^{-1}/(e^5+e^2+e^{-1}+e^3) \\ e^3/(e^5+e^2+e^{-1}+e^3) \end{bmatrix} = \begin{bmatrix} 0.842 \\ 0.042 \\ 0.002 \\ 0.114 \end{bmatrix}$$  

<br>

Softmax这个名称的来源是与所谓**hardmax**对比，hardmax函数会观察$$z$$的元素，然后对于$$z$$中最大元素的输出为1，其它的输出都为0。与之相反，Softmax所做的从$z$到这些概率的映射更为温和。  

在Softmax分类中，我们一般用到的损失函数是$$L(\hat y, y) = - \sum_{j = 1}^{n}{y_{j}log\hat y_{j}}$$  

而$$dz^{[l]} = \hat{y} -y$$

