# 统计语言模型


自然语言处理的一个基本问题就是为其上下文相关的特性建立数学模型，即统计语言模型（Statistical Language Model），它是自然语言处理的基础。


假定$$S$$表示某个有意义的句子，由一连串特定顺序排列的词$$w_1,w_2,...,w_n$$组成，这里$$n$$是句子的长度。现在，我们想知道$$S$$在文本中出现的可能性，即$$S$$的概率:  
$$P(S)=P(w_1,w_2,...,w_n)$$  
利用条件概率的公式，$$S$$这个序列出现的概率等于每一个词出现的概率相乘，于是$$P(S)$$可展开为：  
$$P(w_1,w_2,...,w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1,w_2) \cdots  P(w_n|w_1,w_2,...,w_{n-1})$$  
其中$$P(w_1)$$表示第一个词$$w_1$$出现的概率；$$P(w_2|w_1)$$ 是在已知第一个词的前提下，第二个词出现的概率；以次类推。不难看出，到了词$$w_n$$，它的出现概率取决于它前面所有词。从计算上来看，各种可能性太多，无法实现。所以需要简化模型。  

### 二元模型(Bi-Gram model)  
利用马尔可夫假设：假设任一个词$$w_i$$出现的概率只同它前面的词$$w_{i-1}$$有关，于是问题就变得很简单了。现在，$$S$$出现的概率就变为：  
$$P(S) = P(w_1,w_2,...,w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_2) \cdots P(w_n|w_{n-1}) $$  
### N元模式(N-Gram Model)  
利用n-1阶马尔可夫假设：假设任意一个词$$w_i$$和前面的n-1个词有关，而与更前面的词无关，这样当前词$$w_i$$的概率只取决于前面n-1个词$$P(w_{i-n+1},w_{i-n+2},...,w_{i-1})$$ 。因此：  
$$P(w_i|w_1,w_2,...,w_{i-1})=P(w_i|w_{i-n+1},w_{i-n+2},...,w_{i-1})$$  

当$$n=1$$时，即一元模型，也称unigram，表示一个上下文无关的模型，此时自由参数量级是词典大小$$V$$;   
当$$n=2$$时，即上述的二元模型，此时自由参数量级是词典大小$$V^2$$;  
当$$n=3$$时，是实际应用最多的是三元模型(Tri-Gram Model)，此时自由参数量级是词典大小$$V^3$$;  
当$$n$$大于3，效果的提升就不是很显著了，但资源的耗费却显著增加，很少人用。  

三元或四元甚至更高阶的模型是否能覆盖所有的语言现象呢？答案显然是否定的。这就是马尔可夫假设的局限性，这时要采取其他一些长程的依赖性（Long distance Dependency）来解决这个问题。  


## 条件概率的估算  
根据大数定理，当统计量足够大时，相对频度约等于概率。所以,一般使用频率计数的比例来计算：  
二元模型  
$$P(w_i|w_{i-1}) = \frac{P(w_{i-1}, w_i)}{P(w_{i-1})} \approx \frac{f(w_{i-1}, w_i)}{f(w_{i-1})} = \frac{count(w_{i-1}, w_i)}{count(w_{i-1})}$$  
N元模型  
$$P(w_i|w_1,w_2,...,w_{i-1}) = \frac {P(w_{i-n+1},w_{i-n+2},...,w_{i-1},w_i)} {P(w_{i-n+1},w_{i-n+2},...,w_{i-1})} \approx  \frac {f(w_{i-n+1},w_{i-n+2},...,w_{i-1},w_i)} {f(w_{i-n+1},w_{i-n+2},...,w_{i-1})} = \frac {count(w_{i-n+1},w_{i-n+2},...,w_{i-1},w_i)} {count(w_{i-n+1},w_{i-n+2},...,w_{i-1})} $$  

### 平滑化
　假设有一个词组在训练语料中没有出现过，那么它的频次就为0，但实际上能不能认为它出现的概率为0呢？显然不可以，我们无法保证训练语料的完备性，所以需要作平滑处理：  

**Laplace平滑**  
　这是平滑方法中最为简单粗暴的一个方法，原理就是让每个统计的频数至少为１，做法就是将每个统计频率分子加１，分母加所有统计汉字的个数，这样的话就可以把本来概率为0结果变为一个很小的值，也是比较合理。Laplace平滑又叫Add-one平滑，由此还扩展了Add-k平滑。  
　但是，实际的应用中却会出现一些问题，由于总的统计概率一定是1，那么这些后来增加的概率就必然会造成原来大概率的减小，而且实验证明，这种减小是巨大的，那么就可能会导致结果的不准确。  


**折扣平滑**  
　基本思想是修改训练样本中事件的实际计数，使样本中(实际出现的)不同事件的概率之和小于1，剩余的概率量分配给未见概率。

**古德-图灵(Good-Turing)平滑**  
(待补充--《数学之美》)  

...



