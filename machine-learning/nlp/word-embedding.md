# 词嵌入 (Word Embedding)

Embedding是数学领域的有名词，是指某个对象X被嵌入到另外一个对象Y中，映射 f : X → Y ，例如有理数嵌入实数。

词向量是用来表示词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌入（word embedding）.


### One-Hot 向量

最简单的Word Embedding，是指将所有词排成一列，对于词A，只有在它的位置置1，其他位置置0，维度就是所有词的数目。在实际应用中,一般采用稀疏编码存储,主要采用词的编号。  
这种表示方法一个最大的问题是词之间的语义和语法关系是相互独立的,无法捕捉词与词之间的相似度,也被称为“词汇鸿沟”问题;其次,是维度爆炸问题,随着词典规模的增大,句子构成的词袋模型的维度变得越来越大,矩阵也变得超稀疏,这种维度的爆增,会大大耗费计算资源。  


## word2Vec  
分布式表示最早由Hinton在1986年提出，其基本思想是通过训练将每个词映射成K维实数向量(K一般为模型中的超参数),通过词之间的距离(比如余弦相似度、欧氏距离等)来判断它们之间的语义相似度。  
而word2vec使用的就是这种分布式表示的词向量表示方式。通过训练,可以把对文本内容的处理简化为K维向量空间中的向量运算,而向量空间上的相似度可以用来表示文本语义上的相似度。

word2vec是一款将词表征为实数值向量的高效工具,采用的模型有CBOW(Continuous	Bag-Of-Words,连续的词袋模型)和Skip-Gram两种。  


## GloVe  
用于词表示的全局向量算法（或称为 GloVe）是由 Pennington 等人于 Stanford 大学开发的，这一算法是对于 word2vec 方法的一个扩展，它可以高效地学习到词向量。

经典的向量空间模型的词表示是使用矩阵分解技术（例如潜在语义分析 [LSA，Latent Semantic Analysis]）开发的，这些技术在使用全局文本统计这方面表现优异，但它们并不像 word2vec 那样学习方法一样，能够捕获（词的）意义并在任务中表示出来，比如计算出类比（例如上面描述的国王与女王的例子）。

GloVe 是一种将矩阵分解技术的全局统计（例如 LSA）与 word2vec 中基于局部语境的学习结合起来的方法。

不同于使用窗口来定义局部上下文（word2vec），GloVe 使用了整个文本语料库，语料库中的统计信息用于构造明确的词的上下文或者词的共生矩阵（Co-occurrence matrix）。使用整个文本语料库的结果，是得到一个通常能获得更好的词嵌入的学习模型。